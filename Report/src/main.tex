\documentclass[12pt]{report}
\usepackage{arabicore}
\usepackage{booktabs}
\usepackage{titleps}
\input{setup}

\begin{document}

    \begin{titlepage}
        \centering

        \renewcommand{\arraystretch}{1.1} % Increase row height
        \begin{tabularx}{\textwidth}{@{}m{0.9\textwidth}X@{}}
            \centering \raggedleft \cellcolor{lightgray!25} Αλέξανδρος Ξιάρχος\\ {\footnotesize st1059619@ceid.upatras.gr} & \centering\cellcolor{darkgray}\fontDin \raisebox{-1pt}{\color{white}1059619 \\}
        \end{tabularx}

        \vspace*{12em}
        \begin{headerlight}
            \begin{Din}
                \centering
                    {ΠΑΝΕΠΙΣΤΗΜΙΟ ΠΑΤΡΩΝ \(\cdot\) ΤΜΗΜΑ ΜΗΧΑΝΙΚΩΝ Η/Υ ΚΑΙ ΠΛΗΡΟΦΟΡΙΚΗΣ}
            \end{Din}
        \end{headerlight}

        \begin{headerdark}
            \begin{Din Medium}
                \centering
                \huge \textcolor{white}{ΑΝΑΚΤΗΣΗ ΠΛΗΡΟΦΟΡΙΑΣ}
            \end{Din Medium}
        \end{headerdark}

        \begin{headerlight}
            \begin{Din}
                \centering
                    ΕΡΓΑΣΤΗΡΙΑΚΗ ΑΣΚΗΣΗ \(\cdot\) 2023 \(\textendash\) 2024
            \end{Din}
        \end{headerlight}

        \vspace*{10em}

    \end{titlepage}

    \tableofcontents
    \pagebreak

%   ===================================================================================================================
    \chapter{ΕΙΣΑΓΩΓΗ}
        \section{ΕΠΙΛΟΓΗ ΣΥΣΤΗΜΑΤΩΝ ΣΤΑΘΜΙΣΗΣ TF-IDF}
        Καταρχάς πρέπει να επιλέξουμε δύο συστήματα στάθμισης των βαρών για τους όρους των εγγράφων και των ερωτημάτων.

            \subsection{ΠΡΩΤΟ ΣΥΣΤΗΜΑ ΣΤΑΘΜΙΣΗΣ}
            Το πρώτο σύστημα στάθμισης είναι μια παραλλαγή\footnote{Το σύστημα αναφέρεται ως παραλλαγή των Salton-Buckley για το λόγο ότι δεν έχει συμπεριληφθεί κάποιος παράγοντας κανονικοποίσης, μιας και τα έγγραφα είναι \textit{περίπου} ισομεγέθη (μέσος όρος 350 λέξεις).} του προτεινόμενου ως καλύτερου πλήρως σταθμισμένου συστήματος σύμφωνα με τους Salton-Buckley\footnote{Gerard Salton, Christopher Buckley, Term-weighting approaches in automatic text retrieval, Information Processing \& Management, Volume 24, Issue 5, 1988, Pages 513-523, ISSN 0306-4573}
            {\fontTimes (best fully weighted system)}. \linebreak Θα χρησιμοποιήσουμε την \textbf{απλή συχνότητα εμφάνισης} {\fontTimes (raw term frequency)} για το TF βάρος των εγγράφων,
            \[ \text{\textbf{Σύστημα \#1:}\hspace{5pt}TF}_{\scriptsize \text{εγγράφων}}=\hspace{5pt} f_{i,j} \]
            όπου \(f_{ij}\) οι φορές που ο όρος εμφανίζεται σε ένα έγγραφο, τη \textbf{διπλή 0,5 κανονικοποίηση} για το TF βάρος των ερωτημάτων {\fontTimes(augmented normalized TF)},
            \[ \text{\textbf{Σύστημα \#1:}\hspace{5pt}TF}_{\scriptsize \text{ερωτημάτων}}=\hspace{5pt} 0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}} \]
            και τέλος την \textbf{απλή ανάστροφη συχνότητα εμφάνισης} για το IDF βάρος και των εγγράφων και των ερωτημάτων:
            \[\text{\textbf{Σύστημα \#1:}\hspace{5pt}IDF}_{\scriptsize\begin{matrix}\text{εγγράφων}\\\text{ερωτημάτων}\end{matrix}}=\hspace{5pt} \log{\frac{N}{n_i}} \]
            όπου \(N\) το πλήθος των εγγράφων και \(n_i\) ο αριθμός των εγγράφων στα οποία εμπεριέχεται ο όρος.

            \subsection{ΔΕΥΤΕΡΟ ΣΥΣΤΗΜΑ ΣΤΑΘΜΙΣΗΣ}
            Ως δεύτερο σύστημα στάθμισης θα χρησιμοποιήσουμε το καλύτερα σταθμισμένο πιθανολογικό σύστημα σύμφωνα με τους Salton-Buckley\footnotemark[1]
            {\fontTimes (best weighted probabilistic weight)} με
            \[ \text{\textbf{Σύστημα \#2:}\hspace{5pt}βάρος όρου}_{\scriptsize \text{εγγράφων}}=\hspace{5pt} 0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}}\]
            \[ \text{\textbf{Σύστημα \#2:}\hspace{5pt}βάρος όρου}_{\scriptsize \text{ερωτημάτων}}=\hspace{5pt} \log{\frac{N - n_i}{n_i}} \]

            Και τα δύο αυτά συστήματα στάθμισης έχουν επιφέρει τα ακριβέστερα αποτελέσματα στο σύνολο των συλλογών στα οποία έχουν εξεταστεί
            αλλά και ειδικότερα για ιατρικές {\fontTimes (MED)} συλλογές. Επομένως, συνολικά έχουμε:

            \noindent
            \begin{tblr}{
                colspec={>{\centering\arraybackslash}m{3.5cm}>{\centering\arraybackslash}m{6.1cm}>{\centering\arraybackslash}m{6.1cm}},
                row{2}={bg=lightgray!50}, row{3}={bg=lightgray}, row{1}={bg=black!90,fg=white}}
                Σύστημα στάθμισης & Βάρος όρου εγγράφου &  Βάρος όρου ερωτήματος \\
                1 & \[f_{i,j} \times \log{\frac{N}{n_i}} \] & \[(0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}}) \times \log{\frac{N}{n_i}} \] \\
                2 & \[0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}} \] & \[\log{\frac{N - n_i}{n_i}} \] \\
            \end{tblr}
            \\\\


%   ===================================================================================================================
    \chapter{ΥΛΟΠΟΙΗΣΗ ΜΟΝΤΕΛΩΝ}
        Η εργασία υλοποιήθηκε σε Python χρησιμοποιώντας τις βιβλιοθήκες:

        \begin{tblr}{
            colspec={>{\centering\arraybackslash}m{5cm}>{\centering\arraybackslash}m{10cm}},
            row{2,4,6,8}={bg=lightgray!50}, row{3,5,7,9}={bg=lightgray}, row{1}={bg=black!90,fg=white}}
            Βιβλιοθήκη & Περιγραφή \\
            \textbf{os} & σύνδεση με λειτουργικό σύστημα \\
            \textbf{nltk} & αφαίρεση stopwords, stemming \\
            \textbf{numpy} & υπολογισμός ομοιότητας συνημιτόνου \\
            \textbf{math} & υπολογισμός λογαρίθμων \\
            \textbf{matplotlib} & γραφικές παραστάσεις \\
            \textbf{json} & αποθήκευση-ανάγνωση JSON αρχείων \\
            \textbf{pickle} & αποθήκευση αντικειμένων Python
        \end{tblr}

        Αυτή είναι η δομή των αρχείων κώδικα όπου έχει χωριστεί η υλοποίηση: \\
        \begin{minipage}{.5\textwidth}\centering
            \begin{forest}
              for tree={
                font=\fontCode\small,
                grow'=00,
                child anchor=west,
                parent anchor=south,
                anchor=west,
                calign=first,
                edge path={
                  \noexpand\path [draw, \forestoption{edge}]
                  (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.5pt] {} (.child anchor)\forestoption{edge label};
                },
                before typesetting nodes={
                  if n=1
                    {insert before={[,phantom]}}
                    {}
                },
                fit=band,
                before computing xy={l=15pt},
              }
            [src
              [main.py]
              [tools.py]
              [preprocessing.py]
              [inverted\_index.py]
              [vector\_space\_model.py]
            ]
            \end{forest}
        \end{minipage}%
        \begin{minipage}{.5\textwidth} \centering
            \begin{forest}
                for tree={
                    font=\fontCode\small,
                    grow'=00,
                    child anchor=west,
                    parent anchor=south,
                    anchor=west,
                    calign=first,
                    edge path={
                      \noexpand\path [draw, \forestoption{edge}]
                      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.5pt] {} (.child anchor)\forestoption{edge label};
                    },
                    before typesetting nodes={
                      if n=1
                        {insert before={[,phantom]}}
                        {}
                    },
                    fit=band,
                    before computing xy={l=15pt},
                  }
                [src
                  [colBERT\_preprocessing.py]
                  [colBERT.ipynb]
                  [evaluation\_metrics.py]
                ]
            \end{forest}
        \end{minipage}%

        Η υλοποίηση του colBERT μοντέλου έχει πραγματοποιηθεί στο Google Colab ως Juputer Notebook.

        \section{ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΕΓΓΡΑΦΩΝ \& ΒΟΗΘΗΤΙΚΕΣ ΣΥΝΑΡΤΗΣΕΙΣ}

            Το αρχείο {\fontCode\small tools.py} περιλαμβάνει βοηθητικές συναρτήσεις για κάποιες επαναλαμβανόμενες διαδικασίες της υλοποίησης.
            Περιλαμβάνονται οι συναρτήσεις {\fontCode\small \textbf{get\_docs}()} και {\fontCode\small \textbf{get\_queries}()}.

            Η συνάρτηση {\fontCode\small \textbf{get\_docs}()}, χρησιμοποιώντας την {\fontCode\small os} βιβλιοθήκη
            διαβάζει το πλήθος των αρχείων της βιβλιοθήκης.\footnote{Να σημειωθεί ότι το πλήθος των εγγράφων διαφέρει από την αύξουσα αρίθμησή τους. Συγκεκριμένα έχουμε 1209 έγγραφα αριθμημένα από το {\fontCode\scriptsize 000001} ως {\fontCode\scriptsize 01239}. Με άλλα λόγια υπάρχουν αριθμοί στη συλλογή που δεν αντιστοιχούν σε έγγραφα. Συνεπώς δεν θα μπορούσαμε να χρησιμοποιήσουμε κάποια αριθμητική επανάληψη, για παράδειγμα, για την εισαγωγή των εγγράφων.}
            Η συνάρτηση, αφού αφαιρέσει το escape character {\fontCode\small '\textbackslash n'}, ο οποίος προκύπτει από την μορφολογία των εγγράφων (η κάθε λέξη είναι τοποθετημένη σε διαφορετική γραμμή),
            δημιουργεί \linebreak και επιστρέφει μια λίστα από tuples, με κάθε tuple να αντιστοιχεί σε κάθε αρχείο-έγγραφο. \linebreak Τα tuples έχουν την δομή:
                \begin{graycomment} \centering
                {\fontCode\footnotesize doc\_tuple = ('docID', ['λήμμα\_1', 'λήμμα\_2' ...])}
                \end{graycomment}

            \noindent όπου {\fontCode\small docID} η αρίθμηση του κάθε εγγράφου και {\fontCode\small doc\_term\_n} η κάθε λέξη-λήμμα του εγγράφου. \linebreak
            Η συνάρτηση {\fontCode\small strip()} είναι απαραίτητη για την αφαίρεση των {\fontCode\small \textbackslash n} χαρακτήρων
            που προέκυψαν από την μορφολογία των εγγράφων (κάθε λέξη είναι σε νέα γραμμή).
            Με παρόμοιο τρόπο η συνάρτηση {\fontCode\small \textbf{get\_queries}()} επιστρέφει τη λίστα με τα ερωτήματα της συλλογής.

            Στις συναρτήσεις {\fontCode\small \textbf{preprocess\_collection}()} και {\fontCode\small \textbf{preprocess\_queries}()}
            πραγματοποιείται η προεπεξεργασία των εγγράφων, συγκεκριμένα η αφαίρεση των stopwords και το stemming.

            Χρησιμοποιούμε τη {\fontCode\small nltk} βιβλιοθήκη. Τα {\fontCode\small doc\_tuples} της
            {\fontCode\small get\_docs()} αφού περάσουν από τον {\fontCode\small PorterStemmer} της {\fontCode\small nltk} αποθηκεύονται σε μια λίστα,
            η οποία στη συνέχεια επιστρέφεται.\linebreak
            Αντίστοιχη διαδικασία πραγματοποιείται και για την προεπεξεργασία των ερωτημάτων, στην {\fontCode\small preprocess\_queries()}.

        \section{ΑΝΕΣΤΡΑΜΜΕΝΟ ΕΥΡΕΤΗΡΙΟ}

            Το ανεστραμμένο ευρετήριο δημιουργείται στη συνάρτηση {\fontCode\small \textbf{create\_inverted\_index}()} του αρχείου {\fontCode\small inverted\_index.py}.
            Στη συνάρτηση εισάγονται οι λίστες που δημιουργήθηκαν στις προηγούμενες συναρτήσεις. Τα tuples από τα οποία αποτελούνται, αποθηκεύονται σε ένα dictionary που θα αποτελέσει το ανεστραμμένο ευρετήριο με την εξής δομή:

                \begin{graycomment} \centering
                    {\fontCode\footnotesize inverted\_index['λήμμα'] = \\ \{ ('docID στο οποίο εμφανίζεται' = <φορές εμφάνισης>), (\(\cdots\)), \(\ldots\) \}}
                \end{graycomment}

            Κάθε value του dictionary είναι ένα set\footnote{Έχει επιλεχθεί set για εξοικονόμιση μνήμης, μας και δεν μας ενδιαφέρει η σειρά των tuples.} το οποίο περιλαμβάνει ένα ή περισσότερα tuples
            με το {\fontCode\small docID} και τη συχνότητα εμφάνισης του λήμματος στο συγκεκριμένο έγγραφο.
            Η συχνότητα υπολογίζεται μέσω της {\fontCode\small count()} σε όλο το έγγραφο ανά λήμμα.

            Αυτό είναι ένα παράδειγμα του τελικού ανεστραμμένου ευρετηρίου\footnote{Τα λήμματα έχουν τη stemming μορφή τους.}:

                \begin{graycomment} \centering
                    \fontCode\footnotesize inverted\_index = \{ \(\ldots\) 'coronari': \{('01217', 2), ('00779', 1)\}, \\ 'mobil': \{('00673', 2)\}, 'strain': \{('00179', 7)\}, \(\ldots\) \}
                \end{graycomment}


        \section{ΥΛΟΠΟΙΗΣΗ VECTOR SPACE ΜΟΝΤΕΛΟΥ}

            Η υλοποίηση του vector space μοντέλου γίνεται στο αρχείο {\fontCode\small vector\_space\_model.py}.
            Η συνάρτηση {\fontCode\small \textbf{run\_vsm}(doc\_collection, queries, inverted\_index)}, μέσω μιας επανάληψης ώστε να καλυφθούν όλα τα queries,
            καλεί την συνάρτηση {\fontCode\small \textbf{vsm1}(doc\_collection, query, inverted\_index)} ή {\fontCode\small \textbf{vsm2}()}, ανάλογα με το σύστημα στάθμισης που θέλουμε να χρησιμοποιήσουμε.

            Πριν προχωρήσουμε στις συναρτήσεις {\fontCode\small vsmX()}, από το τρόπο που έχει δομηθεί το ανεστραμμένο ευρετήριο,
            είναι σαφές πως μπορούμε να υπολογίσουμε αμέσως το πλήθος των εγγράφων όπου υπάρχει ένα συγκεκρίμενο λήμμα ως {\fontCode\small len(inverted\_index[term])},
            με άλλα λόγια δηλαδή το \(n_i\). Είναι επίσης προφανές ότι \( N = \) {\fontCode\small len(inverted\_index))}.

            Στην συνάρτηση {\fontCode\small vsm1()} αρχικά υπολογίζονται στο λεξικό {\fontCode\small query\_tfs} οι συχνότητες \(f_{i,j}\) κάθε όρου του εκάστοτε ερωτήματος:
            \begin{graycomment}
                \fontCode\footnotesize query\_tfs\(_{\textit{\fontCode\hspace{2pt}len(query)}}}\)\ = \{ \(\ldots\) 'calcium': 1, 'effect': 2 \(\ldots\) \}
            \end{graycomment}

            Στα λεξικά {\fontCode\small query\_tfidfs} και {\fontCode\small doc\_tfidfs} θα αποθηκευτούν οι τελικές τιμές-βάρη που θα αποτελέσουν τα διανύσματα ερωτήματος και εγγράφων.
            Το λεξικό {\fontCode\small doc\_tfidfs} αρχικοποιείται με άδειες λίστες για κάθε έγγραφο.

            Διατρέχουμε κάθε όρο του ερωτήματος {\fontCode\small query} και αν ο όρος υπάρχει στο ανεστραμμένο ευρετήριο
            υπολογίζουμε την IDF τιμή του στην συνάρτηση {\fontCode\small idfX()}, και στη συνέχεια το TF-IDF βάρος του ερωτήματος στο {\fontCode\small query\_tfidfs}.

           \begin{graycomment}
                \fontCode\footnotesize query\_ifidf\(_{\textit{\fontCode\hspace{2pt}len(query)}}}\)\ = \{ \(\ldots\) 'calcium': 3.6318, 'effect': 1.76483 \(\ldots\) \}
           \end{graycomment}

            Στη συνέχεια διατρέχουμε όλα τα έγγραφα της συλλογής. Αν το έγγραφο περιλαμβάνει τον όρο του ερωτήματος που εξετάζουμε (κάτι που ελέγχουμε από τη λίστα {\fontCode\small docs\_containing\_term}),
            τότε μέσω του ανεστραμμένου ευρετηρίου, αποθηκεύεται η TF τιμή του, υπολογίζεται το TF-IDF βάρος του εγγράφου και αποθηκεύεται στο {\fontCode\small doc\_tfidfs[docID]}.
            Αν το έγγραφο δεν περιλαμβάνει τον όρο, αποθηκεύουμε 0.

            \begin{graycomment}
                \fontCode\footnotesize doc\_ifidf\(_{\textit{\fontCode\hspace{2pt}len(doc\_collection)}}}\)\ = \{'000001': [0, 0, 0.0, 1.76, 1.5]_{\textit{\fontCode len(query)}}} \(\ldots\)\}
            \end{graycomment}

            Εν τέλει έχουμε δημιουργήσει το διάνυσμα ερωτήματος και 1209 διανύσματα εγγράφων, τα οποία έχουν μήκος όσο και το διάνυσμα ερωτήματος. Στην λίστα {\fontCode\small similarity}
            υπολογίζουμε την ομοιότητα συνημιτόνου μεταξύ του διανύσματος ερωτήματος και κάθε διανύσματος εγγράφου. Τέλος, ταξινομούμε την λίστα και επιστρέφουμε τα 100 κείμενα με την μεγαλύτερη ομοιότητα.

            Η ίδια διαδικασία ακολουθείται και στην συνάρτηση {\fontCode\small vsm2()}, με τις απαραίτητες αλλαγές για το διαφορετικό σύστημα στάθμισης.

        \section{ΥΛΟΠΟΙΗΣΗ colBERT}

            Στο αρχείο {\fontCode\small colBERT\_preprocessing.py} γίνεται η προεπεξεργασία των εγγράφων και ερωτημάτων ώστε να δωθούν ως inputs στο colBERT μοντέλο.
            Συγκεκριμένα έγινε μετατροπή των ερωτημάτων σε κεφαλαία και αποθηκεύτηκαν σε JSON αρχεία τα έγγραφα και ερωτήματα σε λεξικά ως εξής:

            \begin{graycomment}
                \fontCode\footnotesize colBERTdocs = \{"docID\(_\text{i}\)": "<doc\(_\text{i}\)>", \(\ldots\) \} \\
                \fontCode\footnotesize colBERTqueries = \{"queryID\(_\text{i}\)": "<query\(_\text{i}\)>", \(\ldots\) \}
            \end{graycomment}

            Η εκτέλεση του colBERT μοντέλου πραγματοποιείται μέσω του αρχείου {\fontCode\small colBERT.ipynb} στο Google Colab.
            Τα λεξικά των JSON αρχείων μετατρέπονται σε 2 ξεχωριστές λίστες (id και περιεχόμενο). Μετά τη δημιουργία του Indexer και του Searcher,
            επιστρέφονται τα 100 κείμενα με την μεγαλύτερη ομοιότητα.

        \chapter{ΜΕΤΡΙΚΕΣ ΑΞΙΟΛΟΓΗΣΗΣ}
            Για την σύγκριση των μοντέλων χρησιμοποιήθηκαν:
            \begin{itemize}
                \item \textbf{Διάγραμμα ανάκλησης-ακρίβειας} {\fontTimes(Precision-Recall curve)}. Το διάγραμμα παρουσιάζει ενδιαφέρον γιατί -συνδυάζοντας τις μετρικές ανάκλησης και ακρίβειας-
            παρουσιάζει με ακρίβεια τα σημεία που το μοντέλο παρουσιάζει μεγαλύτερη ακρίβεια και λιγότερη ανάκληση και αντίστροφα.

                \begin{multicols}{2} \centering
                    \( \text{Ακρίβεια} = \frac{\text{αριθμός σχετικών ανακτηθέντων κειμένων}}{\text{αριθμός εγγράφων που ανακτήθηκαν}} \) \\
                    \( \text{Ανάκληση} = \frac{\text{αριθμός σχετικών ανακτηθέντων κειμένων}}{\text{αριθμός σχετικών εγγράφων στη συλλογή}} \)
                \end{multicols}

                \item \textbf{???????} {\fontTimes (Mean Average Precision \textendash\hspace{1pt} mAP)}. Χρησιμοποιούμε την συγκεκριμένη μετρική λόγω των πολλαπλών ερωτημάτων όπου εφαρμόζουμε τα μοντέλο σε αυτά, για την αξιολόγηση των αποτελεσμάτων.

                    \begin{displaymath}
                        \sum_{i=1}^{i=k}(\text{ανάκληση}[i] - \text{ανάκληση}[i-1])\times\text{ακρίβεια}[i]
                    \end{displaymath}
            \end{itemize}

            \section{\hspace{1pt}ΑΠΟΤΕΛΕΣΜΑΤΑ\hspace{1pt}}


\end{document}
