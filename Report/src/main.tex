\documentclass[12pt]{report}
\usepackage{arabicore}
\usepackage{booktabs}
\usepackage{titleps}
\input{setup}

\begin{document}

    \begin{titlepage}
        \centering

        \renewcommand{\arraystretch}{1.1} % Increase row height
        \begin{tabularx}{\textwidth}{@{}m{0.9\textwidth}X@{}}
            \centering \raggedleft \cellcolor{lightgray!25} Αλέξανδρος Ξιάρχος\\ {\footnotesize st1059619@ceid.upatras.gr} & \centering\cellcolor{darkgray}\fontDin \raisebox{-1pt}{\color{white}1059619 \\}
        \end{tabularx}

        \vspace*{12em}
        \begin{headerlight}
            \begin{Din}
                \centering
                    {ΠΑΝΕΠΙΣΤΗΜΙΟ ΠΑΤΡΩΝ \(\cdot\) ΤΜΗΜΑ ΜΗΧΑΝΙΚΩΝ Η/Υ ΚΑΙ ΠΛΗΡΟΦΟΡΙΚΗΣ}
            \end{Din}
        \end{headerlight}

        \begin{headerdark}
            \begin{Din Medium}
                \centering
                \huge \textcolor{white}{ΑΝΑΚΤΗΣΗ ΠΛΗΡΟΦΟΡΙΑΣ}
            \end{Din Medium}
        \end{headerdark}

        \begin{headerlight}
            \begin{Din}
                \centering
                    ΕΡΓΑΣΤΗΡΙΑΚΗ ΑΣΚΗΣΗ \(\cdot\) 2023 \(\textendash\) 2024
            \end{Din}
        \end{headerlight}

        \vspace*{10em}

    \end{titlepage}

    \tableofcontents
    \pagebreak

%   ===================================================================================================================
    \chapter{ΕΙΣΑΓΩΓΗ}
        \section{ΕΠΙΛΟΓΗ ΣΥΣΤΗΜΑΤΩΝ ΣΤΑΘΜΙΣΗΣ TF-IDF}
        Καταρχάς πρέπει να επιλέξουμε τα δύο συστήματα στάθμισης των βαρών για τα διανύσματα που θα χρησιμοποιήσουμε.

            \subsection{ΠΡΩΤΟ ΣΥΣΤΗΜΑ ΣΤΑΘΜΙΣΗΣ}
            Το πρώτο σύστημα στάθμισης θα είναι μια παραλλαγή\footnote{Το σύστημα αναφέρεται ως παραλλαγή των Salton-Buckley για το λόγο ότι δεν έχει συμπεριληφθεί κάποιος παράγοντας κανονικοποίσης, μιας και τα έγγραφα είναι \textit{περίπου} ισομεγέθη (μέσος όρος 350 λέξεις).} του προτεινόμενου ως καλύτερου πλήρως σταθμισμένου συστήματος σύμφωνα με τους Salton-Buckley\footnote{Gerard Salton, Christopher Buckley, Term-weighting approaches in automatic text retrieval, Information Processing \& Management, Volume 24, Issue 5, 1988, Pages 513-523, ISSN 0306-4573}
            {\fontTimes (best fully weighted system)}. \linebreak Θα χρησιμοποιήσουμε την \textbf{απλή συχνότητα εμφάνισης} {\fontTimes (raw term frequency)} για το TF βάρος των εγγράφων,
            \[ \text{\textbf{Σύστημα \#1:}\hspace{5pt}TF}_{\scriptsize \text{εγγράφων}}=\hspace{5pt} f_{i,j} \]
            όπου \(f_{ij}\) οι φορές που ο όρος εμφανίζεται σε ένα έγγραφο, τη \textbf{διπλή 0,5 κανονικοποίηση} για το TF βάρος των ερωτημάτων {\fontTimes(augmented normalized TF)},
            \[ \text{\textbf{Σύστημα \#1:}\hspace{5pt}TF}_{\scriptsize \text{ερωτημάτων}}=\hspace{5pt} 0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}} \]
            και τέλος την \textbf{απλή ανάστροφη συχνότητα εμφάνισης} για το IDF βάρος και των εγγράφων και των ερωτημάτων:
            \[\text{\textbf{Σύστημα \#1:}\hspace{5pt}IDF}_{\scriptsize\begin{matrix}\text{εγγράφων}\\\text{ερωτημάτων}\end{matrix}}=\hspace{5pt} \log{\frac{N}{n_i}} \]
            όπου \(N\) το πλήθος των εγγράφων και \(n_i\) ο αριθμός των εγγράφων στα οποία εμπεριέχεται ο όρος.

            \subsection{ΔΕΥΤΕΡΟ ΣΥΣΤΗΜΑ ΣΤΑΘΜΙΣΗΣ}
            Ως δεύτερο σύστημα στάθμισης θα χρησιμοποιήσουμε το καλύτερα σταθμισμένο πιθανολογικό σύστημα σύμφωνα με τους Salton-Buckley\footnotemark[1]
            {\fontTimes (best weighted probabilistic weight)} με
            \[ \text{\textbf{Σύστημα \#2:}\hspace{5pt}βάρος όρου}_{\scriptsize \text{εγγράφων}}=\hspace{5pt} 0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}}\]
            \[ \text{\textbf{Σύστημα \#2:}\hspace{5pt}βάρος όρου}_{\scriptsize \text{ερωτημάτων}}=\hspace{5pt} \log{\frac{N - n_i}{n_i}}. \]

            Και τα δύο αυτά συστήματα στάθμισης έχουν επιφέρει τα ακριβέστερα αποτελέσματα και στο σύνολο των συλλογών στα οποία έχουν εξεταστεί
            αλλά και ειδικότερα σε ιατρικές {\fontTimes (MED)} συλλογές. Επομένως, συνολικά έχουμε:

            \noindent
            \begin{tblr}{
                colspec={>{\centering\arraybackslash}m{3.5cm}>{\centering\arraybackslash}m{6.1cm}>{\centering\arraybackslash}m{6.1cm}},
                row{2}={bg=lightgray!50}, row{3}={bg=lightgray}, row{1}={bg=black!90,fg=white}}
                Σύστημα στάθμισης & Βάρος όρου εγγράφου &  Βάρος όρου ερωτήματος \\
                1 & \[f_{i,j} \times \log{\frac{N}{n_i}} \] & \[(0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}}) \times \log{\frac{N}{n_i}} \] \\
                2 & \[0.5 + 0.5 \frac{f_{i,j}}{max_i\hspace{3pt}f_{i,j}} \] & \[\log{\frac{N - n_i}{n_i}} \] \\
            \end{tblr}
            \\\\


%   ===================================================================================================================
    \chapter{ΥΛΟΠΟΙΗΣΗ ΜΟΝΤΕΛΩΝ}
        Η εργασία υλοποιήθηκε σε Python χρησιμοποιώντας τις βιβλιοθήκες:

        \begin{tblr}{
            colspec={>{\centering\arraybackslash}m{5cm}>{\centering\arraybackslash}m{10cm}},
            row{2,4,6,8}={bg=lightgray!50}, row{3,5,7,9}={bg=lightgray}, row{1}={bg=black!90,fg=white}}
            Βιβλιοθήκη & Περιγραφή \\
            \textbf{\texttt{os}} & σύνδεση με λειτουργικό σύστημα \\
            \textbf{\texttt{json}} & αποθήκευση-ανάγνωση JSON αρχείων \\
            \textbf{\texttt{nltk}} & αφαίρεση stopwords, stemming \\
            \textbf{\texttt{numpy}} & υπολογισμός ομοιότητας συνημιτόνου \\
            \textbf{\texttt{math}} & υπολογισμός λογαρίθμων \\
            \textbf{\texttt{matplotlib}} & γραφικές παραστάσεις \\
        \end{tblr}

        Αυτή είναι η δομή των αρχείων κώδικα όπου έχει χωριστεί η υλοποίηση: \\
        \begin{minipage}{.5\textwidth}\centering
            \begin{forest}
              for tree={
                font=\tt,
                grow'=00,
                child anchor=west,
                parent anchor=south,
                anchor=west,
                calign=first,
                edge path={
                  \noexpand\path [draw, \forestoption{edge}]
                  (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.5pt] {} (.child anchor)\forestoption{edge label};
                },
                before typesetting nodes={
                  if n=1
                    {insert before={[,phantom]}}
                    {}
                },
                fit=band,
                before computing xy={l=15pt},
              }
            [src
              [main.py]
              [tools.py]
              [preprocessing.py]
              [inverted\_index.py]
              [vector\_space\_model.py]
            ]
            \end{forest}
        \end{minipage}%
        \begin{minipage}{.5\textwidth} \centering
            \begin{forest}
                for tree={
                    font=\tt,
                    grow'=00,
                    child anchor=west,
                    parent anchor=south,
                    anchor=west,
                    calign=first,
                    edge path={
                      \noexpand\path [draw, \forestoption{edge}]
                      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.5pt] {} (.child anchor)\forestoption{edge label};
                    },
                    before typesetting nodes={
                      if n=1
                        {insert before={[,phantom]}}
                        {}
                    },
                    fit=band,
                    before computing xy={l=15pt},
                  }
                [src
                  [colBERT\_preprocessing.py]
                  [colBERT.ipynb]
                  [evaluation\_metrics.py]
                ]
            \end{forest}
        \end{minipage}%

        Τέλος, η υλοποίηση του colBERT μοντέλου έχει πραγματοποιηθεί στο Google Colab ως Juputer Notebook.

        \section{ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΕΓΓΡΑΦΩΝ \& ΒΟΗΘΗΤΙΚΕΣ ΣΥΝΑΡΤΗΣΕΙΣ}
            
            \subsection{\tt tools.py}

                Το αρχείο \texttt{tools.py} περιλαμβάνει βοηθητικές συναρτήσεις για κάποιες επαναλαμβανόμενες διαδικασίες της υλοποίησης.

                Η συνάρτηση \texttt{\textbf{get\_docs}()}, χρησιμοποιώντας την \texttt{os} βιβλιοθήκη, διαβάζει το πλήθος των αρχείων της συλλογής.\footnote{Να σημειωθεί ότι το πλήθος των εγγράφων διαφέρει από την αύξουσα αρίθμησή τους. Συγκεκριμένα έχουμε 1209 έγγραφα αριθμημένα από το \texttt{000001} ως \texttt{01239}. Με άλλα λόγια υπάρχουν αριθμοί στη συλλογή που δεν αντιστοιχούν σε έγγραφα. Συνεπώς δεν θα μπορούσαμε να χρησιμοποιήσουμε κάποια αριθμητική επανάληψη, για παράδειγμα, για την εισαγωγή των εγγράφων.}
                Η συνάρτηση, αφού αφαιρέσει το escape character \texttt{'\textbackslash n'}, το οποίο προκύπτει από την μορφολογία των εγγράφων (η κάθε λέξη είναι τοποθετημένη σε διαφορετική γραμμή),
                δημιουργεί \linebreak και επιστρέφει μια λίστα από tuples, με κάθε tuple να αντιστοιχεί σε κάθε αρχείο-έγγραφο. \linebreak
                Έτσι τα έγγραφα έχουν τη δομή:
                    \begin{graycomment} \centering
                    \verb|doc = ('docID', ['λήμμα_1', 'λήμμα_2' ...])|
                    \end{graycomment}

                \noindent όπου \texttt{docID} η αρίθμηση του κάθε εγγράφου και \verb|λήμμα_n| η κάθε λέξη-λήμμα του εγγράφου.
                Αυτός θα είναι ο τρόπος αποθήκευσης και προσπέλασης των εγγράφων σε όλη την υλοποίηση.

                Η συνάρτηση \texttt{strip()} της \verb|get_docs()| είναι απαραίτητη για την αφαίρεση των \texttt{\textbackslash n} χαρακτήρων
                που προκύπτουν από τη μορφολογία των εγγράφων (μιας και κάθε λέξη είναι σε νέα γραμμή).
                Με παρόμοιο τρόπο η συνάρτηση \texttt{\textbf{get\_queries}()} επιστρέφει τη λίστα με τα ερωτήματα της συλλογής,
                η \texttt{\textbf{get\_relevant}()} τη λίστα με τα σχετικά έγγραφα ανά ερώτημα και η \texttt{\textbf{get\_json\_file}()} τα περιεχόμενα ενός JSON αρχείου.

            \subsection{\tt preprocessing.py}

                Στο αρχείο \texttt{preprocessing.py}, και συγκεκριμένα στις συναρτήσεις \texttt{\textbf{preprocess\_collection}()} και \texttt{\textbf{preprocess\_queries}()}
                πραγματοποιείται η προεπεξεργασία των εγγράφων, συγκεκριμένα η αφαίρεση των stopwords και το stemming.

                Χρησιμοποιούμε τη \texttt{nltk} βιβλιοθήκη. Κάθε λέξη από τη συλλογή των εγγράφων αφού περάσουν από τον \texttt{PorterStemmer} της \texttt{nltk}
                και ελεγχθούν ότι δεν ανήκουν στη λίστα των stopwords, επιστρέφονται σε παρόμοια μορφή όπως δημιουργήθηκαν στη \verb|get_docs()|.
                Αντίστοιχη διαδικασία πραγματοποιείται και για την προεπεξεργασία των ερωτημάτων, στη \verb|preprocess_queries()|.

        \section{ΑΝΕΣΤΡΑΜΜΕΝΟ ΕΥΡΕΤΗΡΙΟ}

            Το ανεστραμμένο ευρετήριο δημιουργείται στη συνάρτηση \texttt{\textbf{create\_inverted\_index}()} του αρχείου \verb|inverted_index.py|.
            Τα έγγραφα, μετά την προεπεξεργασία τους, εισέρχονται σε μια δομή επανάληψης, \linebreak η οποία δημιουργεί το ανεστραμμένο ευρετήριο ως μορφή λεξικού ως εξής:

                \begin{graycomment} \centering
                    \texttt{\small \textbf{inverted\_index['λήμμα']} = \\ \{('docID\(_\texttt{1}\)': <φορές εμφάνισης>), ('docID\(_\texttt{2}\)': <φορές εμφάνισης>), \(\ldots\) \}}
                \end{graycomment}

            Κάθε value του dictionary είναι ένα σύνολο\footnote{Έχει επιλεχθεί set για εξοικονόμιση μνήμης, μας και δεν μας ενδιαφέρει η σειρά των tuples.} το οποίο περιλαμβάνει ένα ή περισσότερα tuples με το \texttt{docID}
            και τη συχνότητα εμφάνισης του λήμματος στο συγκεκριμένο έγγραφο. Η συχνότητα υπολογίζεται μέσω της \texttt{count()} σε όλο το έγγραφο ανά λήμμα.

            Αυτό είναι ένα παράδειγμα του τελικού ανεστραμμένου ευρετηρίου:

                \begin{graycomment} \centering
                    \tt\small \textbf{inverted\_index} = \{ \(\ldots\) 'coronari': \{('01217', 2), ('00779', 1)\}, \\ 'mobil': \{('00673', 2)\}, 'strain': \{('00179', 7)\}, \(\ldots\) \}
                \end{graycomment}


        \section{ΥΛΟΠΟΙΗΣΗ VECTOR SPACE ΜΟΝΤΕΛΟΥ}

            Η υλοποίηση του vector space μοντέλου γίνεται στο αρχείο {\fontCode\small vector\_space\_model.py}.
            Η συνάρτηση {\fontCode\small \textbf{run\_vsm}(doc\_collection, queries, inverted\_index)}, μέσω μιας επανάληψης ώστε να καλυφθούν όλα τα queries,
            καλεί την συνάρτηση {\fontCode\small \textbf{vsm1}(doc\_collection, query, inverted\_index)} ή {\fontCode\small \textbf{vsm2}()}, ανάλογα με το σύστημα στάθμισης που θέλουμε να χρησιμοποιήσουμε.

            Πριν προχωρήσουμε στις συναρτήσεις {\fontCode\small vsmX()}, από το τρόπο που έχει δομηθεί το ανεστραμμένο ευρετήριο,
            είναι σαφές πως μπορούμε να υπολογίσουμε αμέσως το πλήθος των εγγράφων όπου υπάρχει ένα συγκεκρίμενο λήμμα ως {\fontCode\small len(inverted\_index[term])},
            με άλλα λόγια δηλαδή το \(n_i\). Είναι επίσης προφανές ότι \( N = \) {\fontCode\small len(inverted\_index))}.

            Στην συνάρτηση {\fontCode\small vsm1()} αρχικά υπολογίζονται στο λεξικό {\fontCode\small query\_tfs} οι συχνότητες \(f_{i,j}\) κάθε όρου του εκάστοτε ερωτήματος:
            \begin{graycomment}
                \fontCode\footnotesize query\_tfs\(_{\textit{\fontCode\hspace{2pt}len(query)}}}\)\ = \{ \(\ldots\) 'calcium': 1, 'effect': 2 \(\ldots\) \}
            \end{graycomment}

            Στα λεξικά {\fontCode\small query\_tfidfs} και {\fontCode\small doc\_tfidfs} θα αποθηκευτούν οι τελικές τιμές-βάρη που θα αποτελέσουν τα διανύσματα ερωτήματος και εγγράφων.
            Το λεξικό {\fontCode\small doc\_tfidfs} αρχικοποιείται με άδειες λίστες για κάθε έγγραφο.

            Διατρέχουμε κάθε όρο του ερωτήματος {\fontCode\small query} και αν ο όρος υπάρχει στο ανεστραμμένο ευρετήριο
            υπολογίζουμε την IDF τιμή του στην συνάρτηση {\fontCode\small idfX()}, και στη συνέχεια το TF-IDF βάρος του ερωτήματος στο {\fontCode\small query\_tfidfs}.

           \begin{graycomment}
                \fontCode\footnotesize query\_ifidf\(_{\textit{\fontCode\hspace{2pt}len(query)}}}\)\ = \{ \(\ldots\) 'calcium': 3.6318, 'effect': 1.76483 \(\ldots\) \}
           \end{graycomment}

            Στη συνέχεια διατρέχουμε όλα τα έγγραφα της συλλογής. Αν το έγγραφο περιλαμβάνει τον όρο του ερωτήματος που εξετάζουμε (κάτι που ελέγχουμε από τη λίστα {\fontCode\small docs\_containing\_term}),
            τότε μέσω του ανεστραμμένου ευρετηρίου, αποθηκεύεται η TF τιμή του, υπολογίζεται το TF-IDF βάρος του εγγράφου και αποθηκεύεται στο {\fontCode\small doc\_tfidfs[docID]}.
            Αν το έγγραφο δεν περιλαμβάνει τον όρο, αποθηκεύουμε 0.

            \begin{graycomment}
                \fontCode\footnotesize doc\_ifidf\(_{\textit{\fontCode\hspace{2pt}len(doc\_collection)}}}\)\ = \{'000001': [0, 0, 0.0, 1.76, 1.5]_{\textit{\fontCode len(query)}}} \(\ldots\)\}
            \end{graycomment}

            Εν τέλει έχουμε δημιουργήσει το διάνυσμα ερωτήματος και 1209 διανύσματα εγγράφων, τα οποία έχουν μήκος όσο και το διάνυσμα ερωτήματος. Στην λίστα {\fontCode\small similarity}
            υπολογίζουμε την ομοιότητα συνημιτόνου μεταξύ του διανύσματος ερωτήματος και κάθε διανύσματος εγγράφου. Τέλος, ταξινομούμε την λίστα και επιστρέφουμε τα 100 κείμενα με την μεγαλύτερη ομοιότητα.

            Η ίδια διαδικασία ακολουθείται και στην συνάρτηση {\fontCode\small vsm2()}, με τις απαραίτητες αλλαγές για το διαφορετικό σύστημα στάθμισης.

        \section{ΥΛΟΠΟΙΗΣΗ colBERT}

            Στο αρχείο {\fontCode\small colBERT\_preprocessing.py} γίνεται η προεπεξεργασία των εγγράφων και ερωτημάτων ώστε να δωθούν ως inputs στο colBERT μοντέλο.
            Συγκεκριμένα έγινε μετατροπή των ερωτημάτων σε κεφαλαία και αποθηκεύτηκαν σε JSON αρχεία τα έγγραφα και ερωτήματα σε λεξικά ως εξής:

            \begin{graycomment}
                \fontCode\footnotesize colBERTdocs = \{"docID\(_\text{i}\)": "<doc\(_\text{i}\)>", \(\ldots\) \} \\
                \fontCode\footnotesize colBERTqueries = \{"queryID\(_\text{i}\)": "<query\(_\text{i}\)>", \(\ldots\) \}
            \end{graycomment}

            Η εκτέλεση του colBERT μοντέλου πραγματοποιείται μέσω του αρχείου {\fontCode\small colBERT.ipynb} στο Google Colab.
            Τα λεξικά των JSON αρχείων μετατρέπονται σε 2 ξεχωριστές λίστες (id και περιεχόμενο). Μετά τη δημιουργία του Indexer και του Searcher,
            επιστρέφονται τα 100 κείμενα με την μεγαλύτερη ομοιότητα.

        \chapter{ΜΕΤΡΙΚΕΣ ΑΞΙΟΛΟΓΗΣΗΣ}
            Για την σύγκριση των μοντέλων χρησιμοποιήθηκαν:
            \begin{itemize}
                \item \textbf{Διάγραμμα ανάκλησης-ακρίβειας} {\fontTimes(Precision-Recall curve)}. Το διάγραμμα παρουσιάζει ενδιαφέρον γιατί -συνδυάζοντας τις μετρικές ανάκλησης και ακρίβειας-
            παρουσιάζει με ακρίβεια τα σημεία που το μοντέλο παρουσιάζει μεγαλύτερη ακρίβεια και λιγότερη ανάκληση και αντίστροφα.

                \begin{multicols}{2} \centering
                    \( \text{Ακρίβεια} = \frac{\text{αριθμός σχετικών ανακτηθέντων κειμένων}}{\text{αριθμός εγγράφων που ανακτήθηκαν}} \) \\
                    \( \text{Ανάκληση} = \frac{\text{αριθμός σχετικών ανακτηθέντων κειμένων}}{\text{αριθμός σχετικών εγγράφων στη συλλογή}} \)
                \end{multicols}

                \item \textbf{???????} {\fontTimes (Mean Average Precision \textendash\hspace{1pt} mAP)}. Χρησιμοποιούμε την συγκεκριμένη μετρική λόγω των πολλαπλών ερωτημάτων όπου εφαρμόζουμε τα μοντέλο σε αυτά, για την αξιολόγηση των αποτελεσμάτων.

                    \begin{displaymath}
                        \sum_{i=1}^{i=k}(\text{ανάκληση}[i] - \text{ανάκληση}[i-1])\times\text{ακρίβεια}[i]
                    \end{displaymath}
            \end{itemize}

            \section{\hspace{1pt}ΑΠΟΤΕΛΕΣΜΑΤΑ\hspace{1pt}}


\end{document}
